{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/629 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "==> Preparing data..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 629/629 [00:21<00:00, 28.93it/s]\n",
      "  2%|▏         | 5/231 [00:00<00:04, 47.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining data includes 11 classes, 629 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 231/231 [00:05<00:00, 45.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTesting data includes 12 classes (Original 13 classes), 231 samples.\n",
      "\tDuring testing, openness is 0.061916848035314054.\n",
      "==> Resuming from checkpoint..\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import pretrainedmodels\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchnet\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "#from models import *\n",
    "sys.path.append(\"../..\")\n",
    "import backbones.cifar as models\n",
    "from datasets import CellDataset\n",
    "from Utils import adjust_learning_rate, progress_bar, Logger, mkdir_p, Evaluation\n",
    "from openmax import compute_train_score_and_mavs_and_dists,fit_weibull,openmax\n",
    "\n",
    "model_names = sorted(name for name in models.__dict__\n",
    "    if not name.startswith(\"__\")\n",
    "    and callable(models.__dict__[name]))\n",
    "\n",
    "os.environ[\"HDF5_USE_FILE_LOCKING\"] = \"FALSE\"\n",
    "\n",
    "\n",
    "lr = 0.001\n",
    "resume = './checkpoints/cell_unknown2/xception/last_model.pth'\n",
    "includes_all_train_class = True\n",
    "weibull_tail =20\n",
    "weibull_alpha =3\n",
    "weibull_threshold =0.9\n",
    "# args = parser.parse_args()\n",
    "arch = \"xception\"\n",
    "test_class_num = 2\n",
    "train_class_num = 11\n",
    "bs = 10\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0 \n",
    "\n",
    "def test(epoch, net,trainloader,  testloader,criterion, device,testset):\n",
    "    net.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    #print(net.module)\n",
    "    scores, labels = [], []\n",
    "    net_features = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            # loss = criterion(outputs, targets)\n",
    "            # test_loss += loss.item()\n",
    "            # _, predicted = outputs.max(1)\n",
    "            scores.append(outputs)\n",
    "            labels.append(targets)\n",
    "            net_features.append(net.module.features(inputs))\n",
    "            # total += targets.size(0)\n",
    "            # correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            progress_bar(batch_idx, len(testloader))\n",
    "\n",
    "    plot_features = torch.cat(net_features, dim=0).cpu().numpy()\n",
    "    print(plot_features.shape)\n",
    "    # Get the prdict results.\n",
    "    scores = torch.cat(scores,dim=0).cpu().numpy()\n",
    "    labels = torch.cat(labels,dim=0).cpu().numpy()\n",
    "    scores = np.array(scores)[:, np.newaxis, :]\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    return features\n",
    "    # Fit the weibull distribution from training data.\n",
    "#     print(\"Fittting Weibull distribution...\")\n",
    "#     _, mavs, dists = compute_train_score_and_mavs_and_dists(train_class_num, trainloader, device, net)\n",
    "#     categories = list(range(0, train_class_num))\n",
    "#     weibull_model = fit_weibull(mavs, dists, categories, weibull_tail, \"euclidean\")\n",
    "\n",
    "#     pred_softmax, pred_softmax_threshold, pred_openmax = [], [], []\n",
    "#     for score in scores:\n",
    "#         so, ss = openmax(weibull_model, categories, score,\n",
    "#                          0.5, weibull_alpha, \"euclidean\")  # openmax_prob, softmax_prob\n",
    "#         # for soft max probability confience < 0.4 as unseen\n",
    "#         pred_softmax.append(np.argmax(ss) if np.max(ss) >= 0.2 else train_class_num)\n",
    "#         pred_softmax_threshold.append(np.argmax(ss) if np.max(ss) >= weibull_threshold else train_class_num)\n",
    "#         pred_openmax.append(np.argmax(so) if np.max(so) >= weibull_threshold else train_class_num)\n",
    "\n",
    "#     print(\"Evaluation...\")\n",
    "#     eval_softmax = Evaluation(pred_softmax, labels)\n",
    "#     eval_softmax_threshold = Evaluation(pred_softmax_threshold, labels)\n",
    "#     eval_openmax = Evaluation(pred_openmax, labels)\n",
    "\n",
    "\n",
    "#     label_names=[]\n",
    "\n",
    "#     with open(testset.label_id_path_file, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         label_names = [line.rstrip() for line in lines]\n",
    "\n",
    "#     label_names.append('unseen')\n",
    "\n",
    "\n",
    "\n",
    "#     print(f\"Softmax accuracy is %.3f, auc is %.3f ,f1_score is %.3f\"%(eval_softmax.accuracy,\n",
    "#                                                                       eval_softmax.area_under_roc(),eval_softmax.f1_macro))\n",
    "#     print(f\"Softmax-with-threshold accuracy is %.3f, auc is %.3f ,f1_score is %.3f\"%(eval_softmax_threshold.accuracy,\n",
    "#                                                                                      eval_softmax_threshold.area_under_roc(),eval_softmax_threshold.f1_macro))\n",
    "#     print(f\"Openmax accuracy is %.3f, auc is %.3f,f1_score is %.3f\"%(eval_openmax.accuracy,eval_openmax.area_under_roc(),eval_openmax.f1_macro))\n",
    "\n",
    "#     import matplotlib.pyplot as plt\n",
    "    \n",
    "\n",
    "#     fig,axs=plt.subplots(nrows=1,ncols=3)\n",
    "#     eval_softmax.plot_confusion_matrix(ax=axs[0],labels=label_names,)\n",
    "#     axs[0].set_title('softmax')\n",
    "#     eval_softmax_threshold.plot_confusion_matrix(ax=axs[1],labels=label_names)\n",
    "#     axs[1].set_title('softmaxWiththreshold')\n",
    "#     eval_openmax.plot_confusion_matrix(ax=axs[2],labels=label_names)\n",
    "#     axs[2].set_title('openmax')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# checkpoint\n",
    "checkpoint = './checkpoints/cell_unknown{}/'.format(test_class_num) + arch\n",
    "if not os.path.isdir(checkpoint):\n",
    "    mkdir_p(checkpoint)\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "\n",
    "    #transforms.RandomCrop(112,pad_if_needed=True,padding=4),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    #transforms.RandomCrop(112),\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "trainset = CellDataset(\n",
    "    train_data_path=\"../../../OSR_DATASETS/OSR_Cell_Data_unknown{}/train\".format(test_class_num),\n",
    "    unknown_class=test_class_num, train=True, transform=transform_train)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=bs, shuffle=True, num_workers=4)\n",
    "testset = CellDataset(\n",
    "    train_data_path=\"../../../OSR_DATASETS/OSR_Cell_Data_unknown{}/val\".format(test_class_num),\n",
    "    unknown_class=test_class_num, train=False, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=bs, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Model\n",
    "\n",
    "if arch == 'xception':\n",
    "\n",
    "    net = pretrainedmodels.__dict__[arch](num_classes=train_class_num,pretrained=None)\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "if resume:\n",
    "    # Load checkpoint.\n",
    "    if os.path.isfile(resume):\n",
    "\n",
    "        print('==> Resuming from checkpoint..')\n",
    "        checkpoint = torch.load(resume)\n",
    "        net.load_state_dict(checkpoint['net'])\n",
    "        # best_acc = checkpoint['acc']\n",
    "        # print(\"BEST_ACCURACY: \"+str(best_acc))\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        #logger = Logger(os.path.join(checkpoint, 'log.txt'), resume=True)\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==============================================================>..]  Step: 71ms | Tot: 2s653ms 24/24  \n",
      "(231, 2048)\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "#print(net.module)\n",
    "scores, labels = [], []\n",
    "net_features = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        # loss = criterion(outputs, targets)\n",
    "        # test_loss += loss.item()\n",
    "        # _, predicted = outputs.max(1)\n",
    "        scores.append(outputs)\n",
    "        labels.append(targets)\n",
    "        net_features.append(net.module.features(inputs))\n",
    "        # total += targets.size(0)\n",
    "        # correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(testloader))\n",
    "\n",
    "plot_features = torch.cat(net_features, dim=0).cpu().numpy()\n",
    "print(plot_features.shape)\n",
    "# Get the prdict results.\n",
    "scores = torch.cat(scores,dim=0).cpu().numpy()\n",
    "labels = torch.cat(labels,dim=0).cpu().numpy()\n",
    "scores = np.array(scores)[:, np.newaxis, :]\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "predict_score= []\n",
    "for score in scores:\n",
    "    softmax_prob = softmax(np.array(score.ravel()))\n",
    "    predict_score.append(softmax_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [===============================================================>.]  Step: 72ms | Tot: 7s777ms 63/63  ......]  Step: 72ms | Tot: 203ms 3/63 ===========================================>..........]  Step: 61ms | Tot: 7s49ms 54/63 \n",
      "(629, 2048)\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "#print(net.module)\n",
    "train_scores, train_labels = [], []\n",
    "train_features = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = net(inputs)\n",
    "        # loss = criterion(outputs, targets)\n",
    "        # test_loss += loss.item()\n",
    "        # _, predicted = outputs.max(1)\n",
    "        train_scores.append(outputs)\n",
    "        train_labels.append(targets)\n",
    "        train_features.append(net.module.features(inputs))\n",
    "        # total += targets.size(0)\n",
    "        # correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader))\n",
    "\n",
    "train_features = torch.cat(train_features, dim=0).cpu().numpy()\n",
    "print(train_features.shape)\n",
    "# Get the prdict results.\n",
    "train_scores = torch.cat(train_scores,dim=0).cpu().numpy()\n",
    "train_labels = torch.cat(train_labels,dim=0).cpu().numpy()\n",
    "train_scores = np.array(train_scores)[:, np.newaxis, :]\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False False False  True False False False]\n",
      " [False False False False False False False False False False False]\n",
      " [False False False False False False False False False  True False]\n",
      " [False False False False False False False False False False False]\n",
      " [False  True False False False False False False False False False]\n",
      " [False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False]\n",
      " [False False False False False False False  True False False False]\n",
      " [False False False False False False False False False False False]\n",
      " [False False False False False False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from nonconformist.cp import IcpClassifier\n",
    "from nonconformist.nc import NcFactory\n",
    "    \n",
    "# iris = load_iris()\n",
    "# idx = np.random.permutation(iris.target.size)\n",
    "\n",
    "# Divide the data into proper training set, calibration set and test set\n",
    "#idx_train, idx_cal = idx[:50], idx[50:100]\n",
    "\n",
    "model = SVC(probability=True)\t# Create the underlying model\n",
    "nc = NcFactory.create_nc(model)\t# Create a default nonconformity function\n",
    "icp = IcpClassifier(nc)\t\t\t# Create an inductive conformal classifier\n",
    "\n",
    "# Fit the ICP using the proper training set\n",
    "icp.fit(train_features, train_labels)\n",
    "\n",
    "# Calibrate the ICP using the calibration set\n",
    "icp.calibrate(train_features, train_labels)\n",
    "\n",
    "# Produce predictions for the test set, with confidence 95%\n",
    "prediction = icp.predict(plot_features, significance=0.05)\n",
    "\n",
    "# Print the first 5 predictions\n",
    "pred_labels = []\n",
    "for pred_idx,pred in enumerate(prediction):\n",
    "    \n",
    "    max_idx = np.argmax(predict_score[pred_idx])\n",
    "    max_score = np.max(predict_score[pred_idx])\n",
    "    if max_score >= 0.2 and pred[max_idx] == True:\n",
    "        pred_labels.append(max_idx)\n",
    "    else:\n",
    "        pred_labels.append(train_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7, 11,  9, 11,  1, 11,  3,  7,  3, 11,  2,  1,  1, 10, 11, 11,  9,\n",
       "       11, 11,  1,  7,  6,  5,  5,  6,  1, 11, 10,  9,  2,  9,  8, 11,  9,\n",
       "        6, 10,  2, 11,  5,  4, 11,  8, 11,  2,  9, 10, 11, 10,  8,  6,  0,\n",
       "        7, 11,  5,  6, 11,  9,  8,  6,  7,  0,  1,  0,  5, 11,  7,  7, 11,\n",
       "        7,  1, 11,  7,  5,  2,  9,  8,  3,  6,  8,  5,  1, 11, 11, 11,  6,\n",
       "        8, 11, 11, 11, 11,  6,  4,  9,  0,  8,  1,  7,  2, 11, 11, 11, 11,\n",
       "        0,  2, 11, 11,  4,  1,  9,  8,  7,  2,  9,  5,  9,  9, 11,  6, 11,\n",
       "        6,  1,  8, 11,  6,  9,  7,  3, 11,  9,  1, 11, 11, 11,  0,  2,  4,\n",
       "       10, 11,  9, 11,  8, 11,  5,  7,  6,  0,  6,  0, 11, 11,  5,  1, 11,\n",
       "        8,  5, 11,  9,  5,  5, 11,  1, 11, 11,  9,  9,  5,  3,  2, 11,  4,\n",
       "        7, 11, 11,  1,  9,  4,  2,  1,  9,  2, 11,  7,  3,  9,  9,  4, 11,\n",
       "        9, 11, 11,  9, 11, 11, 11, 11, 11, 11,  7, 10,  9,  1,  4, 11, 11,\n",
       "       11, 11,  4,  9, 11,  6,  5,  9,  1, 11, 11, 10,  2,  1,  6,  8,  4,\n",
       "        5,  4,  2, 11,  8,  1,  7,  5,  3,  6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "eval_softmax = Evaluation(pred_labels, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8441558441558441"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_softmax.accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
